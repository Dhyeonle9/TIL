# 부스팅 트리모델 
# XGBoost

### 부스팅

- 순차적으로 트리를 만들어 이전 트리로부터 더 나은 트리를 만들어내는 알고리즘
    - **랜덤 포레스트**: 각 트리를 독립적으로 만드는 알고리즘
- 트리 모델을 기반으로 한 최신 알고리즘 중 하나로, `랜덤 포레스트보다 훨씬 빠른 속도와 더 좋은 예측` 능력을 보여줌.
    - XG부스트(XGBoost), 라이트GBM(LightGBM) …
    - XGBoost(eXtra Gradient Boost)가 가장 먼저 개발 &가장 널리 활용 (손실 함수뿐만 아니라 모형 복잡도까지 고려) - `경사 하강법 씀
`
---

- 랜덤 포레스트에서 그 다음 세대로 진화하게 되는 중요한 개념.
- 랜덤 포레스트에서는 각각의 트리를 독립적으로, 즉 서로 관련 없이 만드는 반면, 부스팅 알고리즘에서는 트리를 순차적으로 만들면서 `이전 트리에서 학습한 내용이 다음 트리를 만들 때 반영`
- 오버피팅 (과최적화) 문제가 있을 수 있음

### ****XGBoost****

- 캐글 컴피티션 우승자가 많이 사용하는, `성능이 검증된 부스팅 모델`
- 가장 인기 있는 모델로 구글 검색에서 수많은 참고자료 존재
- 종속변수가 연속형 데이터인 경우든 범주형 데이터인 경우든 모두 사용할 수 있음
- 이미지나 자연어가 아닌 `표로 정리된 데이터`의 경우, 거의 모든 상황에 활용할 수 있음
- `회귀/분류 모두 사용가능`
---

**장점**

- 예측 속도가 상당히 빠르며, `예측력 또한 좋음`
- 변수 종류가 많고 데이터가 클수록 상대적으로 뛰어난 성능을 보여줌

**단점**

- 복잡한 모델인 만큼, 해석에 어려움이 있음
- 더 나은 성능을 위한 하이퍼파라미터 튜닝이 까다로움

### 실습


### 부스팅 알고리즘

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/84b5f359-abf3-4090-a2c4-631cebe6dc23/ffbcdd28-51e0-42d5-8c7e-8847229bc43a/Untitled.png)

### 트리 모델의 진화 과정

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/84b5f359-abf3-4090-a2c4-631cebe6dc23/139fbfc1-1680-4c99-9798-a449da45d008/Untitled.png)

### ****배깅 (bagging)****

- 부트스트랩(bootstrap) 훈련셋을 사용하는 트리 모델
- 부트스트랩 : 데이터의 일부분을 무작위로 반복 추출
    - 이러한 식으로 추출한 데이터의 여러 부분집합을 사용해 여 러 트리를 만들어 오버피팅을 방지
    - 이 방법은 랜덤 포레스트에도 포함
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/84b5f359-abf3-4090-a2c4-631cebe6dc23/9dae4c74-8df1-4de9-bd8f-b4170983098b/Untitled.png)
    

### 부스팅 (Boosting)

- 랜덤 포레스트에서 한 단계 더 발전 (여러 트리를 만드는 모델)
- 랜덤 포레스트의 각 트리는 독립적 <-> 부스팅에서는 그렇지 않음.
    - **랜덤 포레스트**: 각 트리를 만들 때 이전에 만든 트리와 상관없이 새로운 데이터 부분집합과 변수 부분집합을 이용
    - **부스팅** : 각 트리를 순차적으로 만들면서 이전 트리 정보 이용 -> 부분집합을 이용해 첫 트리를 만들고 난 후, 해당 트리의 예측 결과를 반영하여 두 번째 트리를 만들어서 첫 번째 트리와의 시너지 효과를 키움

****에이다부스트(AdaBoost, Adaptive Boosting)****

- 부스팅의 대표 알고리즘.
- 단계적으로 트리를 만들 때 이전 단계에서의 분류 결과에 따라 각 데이터에 가중치를 부여/수정.
    - 예) 이전 트리에서 가중치가 덜 부여되고 잘못 분류된 데이터들에 더 높은 가중치를 부여하고, 후속 트리에 서는 가중치가 높은 데이터를 분류하는 데 우선순위
- 이러한 방식으로 트리 여러 개를 만들면 분류가 복잡한 데이터셋도 세부적으로 나눌 수 있는 모델이 만들어짐

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/84b5f359-abf3-4090-a2c4-631cebe6dc23/d1aebe0d-017b-4c09-ae34-f8b64be1a27f/Untitled.png)

### ****경사 부스팅 (gradient boosting)****
- 에이다부스트가 각 데이터에 가중치를 부여/수정하는 방식으로 트리를 만드는 반면, 경사 부스팅은 경사하강법을 이용하여, 이전 모델의 에러를 기반으로 다음 트리를 만들어 감
    - XGBoost, LightGBM, Catboost …
    ****XGBoost (Extreme Gradient Boosting)****

- XGBoost가 기존의 경사 부스팅보다 특별한 이유는 계산 성능 최적화와 알고리즘 개선을 함께 이루었기 때문.
- **계산 성능 최적화**
    - XGBoost 이전의 부스팅 모델은 트리를 순차적으로 만들어내기 때문에 모델링 속도가 느림
    - XGBoost도 마찬가지로 순차적으로 트리를 만들지만, 병렬화, 분산 컴퓨팅, 캐시 최적화 등을 활용해 계산 속도가 훨씬 빠름
- **알고리즘 개선**
    - 경사하강법보다 더 발전된 형태로 최솟값을 찾아냄.
    - 기존 경사하강법이 접점의 기울기를 계산하고 매개변수를 이동한 반면, XGBoost에서는 2차 도함수(2번 미분한 함수)를 활용해 더 적절한 이동 방향과 이동 크기를 찾아내어 더 빠른 시간에 전역 최솟값에 도달
- **정규화 하이퍼파라미터** **지원**
    - 트리 모델이 진화할수록 더 좋은 예측 성능을 보이는 동시에 반대급부로 오버피팅 문제가 더 심각해질 수 있음.
    - XGBoost는 이러한 부작용을 줄일 목적으로 LASSO(L1)와 Ridge(L2) 정규화 하이퍼 파라미터를 지원
    - 그 밖에도 애매하게 예측된 관측치에 높은 가중치를 부여하는 가중치 분위수 스케치, 결측치를 유연하게 처리해내는 희소성 인식 등을 포함하여 성능을 개선


# LightGBM

### ****LightGBM****

- XGBoost 이후로 나온 최신 부스팅 모델
- LightGBM이 등장하기 전까지는 XGBoost가 가장 인기 있는 부스팅 모델이였지만, 점점 LightGBM이 XGBoost와 비슷한 수준 혹은 그 이상 으로 활용되는 추세
- 캐글 컴피티션에서도 좋은 퍼포먼스를 많이 보여주어서 그 성능을 인정받음.
- **리프 중심 트리 분할 방식**을 사용

---

- 종속변수가 연속형 데이터인 경우든 범주형 데이터인 경우든 모두 사용할 수 있음
- 이미지나 자연어가 아닌 표로 정리된 데이터라면 거의 모든 상황에서 활용할 수 있음

---

**장점**

- XGBoost보다 `빠르고, 높은 정확도`를 보이는 경우가 많음
- 예측에 영향을 미친 변수의 중요도를 확인할 수 있음
- `변수 종류가 많고 데이터가 클수록 상대적으로 뛰어난 성능`을 보여줌

**단점**

- 복잡한 모델인 만큼, `해석에 어려움`이 있음
- 하이퍼파라미터 튜닝이 까다로움

### 실습


### ****LightGBM와 XGBoost의 차이점****
LightGBM : `리프 중심 트리 분할 방식`

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/84b5f359-abf3-4090-a2c4-631cebe6dc23/a17c02c8-b2d4-4c30-8fe3-e1e0fa4d7e8c/Untitled.png)

- 핵심 차이 : 트리의 가지를 어떤 식으로 뻗어나가는가?
- XGBoost
    - 균형 분할(Level-wise tree growth)
    - 각 노드에서 같은 깊이를 형성하도록 한층씩 밑으로 내려옴
- LightGBM
    - LightGBM은 이러한 전제를 거부
    - 특정 노드에서 뻗어나가는 가지가 모델의 개선에 더 도움이 된다면 계속하여 진행

****XGBoost (균형 분할 방식)****

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/84b5f359-abf3-4090-a2c4-631cebe6dc23/d705dc9d-e11f-44b3-80c6-05c8c2241c80/Untitled.png)

****LightGBM (리프 중심 분할 방식)****

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/84b5f359-abf3-4090-a2c4-631cebe6dc23/4129c205-b862-425f-8825-7131972db137/Untitled.png)

---

- (앞선 차이점으로) LightGBM은 `속도가 훨씬 빠르게 학습`이 진행될 수 있으며, 대신 복잡성은 더 증가하고, `오버피팅 문제를 야기할 가능성 또한 더 높임`
    - 이러한 문제점은 `하이퍼파라미터 튜닝`으로 어느정도 극복 가능
- 속도 면에서 항상 LightGBM이 XGBoost에 비해 월등히 앞선다?
    - But 이것은 모델링에 `CPU를 활용`하는 것을 전제로 함. 만약 GPU를 사용한다면 XGBoost가 더 빠른 속도를 보임
        
        (LightGBM의 기본 패키지에서는 GPU를 지원하지 않아 별도의 추가 패키지를 설치해야 하는 번거로움이 있음)
        
    - 그러나 딥러닝 알고리즘이 아닌 때는 CPU를 사용하여 모델링하는 경우가 많기 때문에, 전반적으로 LightGBM이 XGBoost보다 나은 성능을 보여줌

---

****XGBoost 대비 LightGBM의 장점****

- 빠른 학습 및 예측
- 더 적은 메모리 사용
- 데이터셋 자동 변환 및 최적 분할